{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.Package Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os #For managing the operating system\n",
    "import requests #For getting files\n",
    "import math\n",
    "\n",
    "import numpy as np #For arrays #Math and matrices\n",
    "import pandas as pd #Gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import seaborn as sns\n",
    "\n",
    "#% matplotlib inline\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "from langchain import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\flori\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\flori\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\flori\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\flori\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "# openai.api_key = ''\n",
    "OPENAI_API_KEY = ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Find Best LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda parameter tuning\n",
    "\n",
    "# Topics range\n",
    "min_topics = 5\n",
    "max_topics = 20 #12\n",
    "\n",
    "rnd_seed = 42\n",
    "np.random.seed(rnd_seed) ## P0066 and P0007 didn't use this seed. \n",
    "\n",
    "model_results = {'Topics': [],\n",
    "                 'Coherence': []\n",
    "                #  'perplexity': []\n",
    "                 }\n",
    "\n",
    "# # Can take a long time to run\n",
    "# pbar = tqdm(total=10)\n",
    "\n",
    "# iterate through validation corpuses\n",
    "for i in range(min_topics, max_topics):\n",
    "    # get the coherence score for the given parameters ##TODO: don't use function, build lda model, calculate perplexity and cv following that. Record perplexity as well. \n",
    "      \n",
    "        lda_model = gensim.models.ldamodel.LdaModel(corpus=bow_corpus,\n",
    "                                           id2word= id2word,\n",
    "                                           num_topics= i, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "        coherence_model_lda = CoherenceModel(\n",
    "            model=lda_model, texts=texts, dictionary=id2word, coherence='u_mass')  \n",
    "        cv = coherence_model_lda.get_coherence()\n",
    "\n",
    "        # perplexity = lda_model.log_perplexity(bow_corpus)\n",
    "                                \n",
    "        # Save the model results\n",
    "        model_results['Topics'].append(i)\n",
    "        model_results['Coherence'].append(cv)\n",
    "        # model_results['perplexity'].append(perplexity)\n",
    "        \n",
    "        # pbar.update(1)\n",
    "# pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save results\n",
    "res_dir = 'C:/dev/'#'D:/dev/Python/data/results/'\n",
    "res_name = 'lda_tuning_results_' + PSO[k] + '.csv'\n",
    "pd.DataFrame(model_results).to_csv(os.path.join(res_dir, res_name), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "# pick the model that gave the highest CV\n",
    "\n",
    "plt.figure()\n",
    "# for valid in valid_set:\n",
    "#     selected_data = results_for_vis.loc[results_for_vis['Validation_Set'] == valid]\n",
    "#     plt.plot(selected_data['Topics'], selected_data['Coherence'], label=valid)\n",
    "\n",
    "plt.plot(model_results['Topics'], model_results['Coherence'], label = 'Coherence')\n",
    "# plt.plot(model_results['Topics'], model_results['perplexity'], label = 'perplexity')\n",
    "\n",
    "# plt.ylabel('Coherence Score')\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.legend()\n",
    "\n",
    "fig_coh_name =  PSO[k] + '_coh.png'\n",
    "print(fig_coh_name)\n",
    "plt.savefig(res_dir + '/' + fig_coh_name)\n",
    "# plt.savefig('P0066_coh.png')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test number of topics\n",
    "## Test the one with the lowest u_mass coherence score for each PSO\n",
    "\n",
    "num_topics = 11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=bow_corpus,\n",
    "                                           id2word= id2word,\n",
    "                                           num_topics= num_topics, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.show_topics(num_topics=-1) ## show all topics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Document topic and percent contribution for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what is the Dominant topic and its percentage contribution in each document\n",
    "def format_topics_sentences(corpus, cleaned_texts, ldamodel=None):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list\n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series(\n",
    "                    [int(topic_num), round(prop_topic, 4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic',\n",
    "                              'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(cleaned_texts)\n",
    "    # orig_texts = pd.Series(orig_texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    # sent_topics_df = pd.concat([sent_topics_df, orig_texts], axis=1)\n",
    "\n",
    "    ## Generate output dataframe\n",
    "    text_orig = text_original.reset_index()['ANSWER_TEXT']\n",
    "    df_topic_sents_keywords = pd.concat([sent_topics_df, text_orig], axis = 1)\n",
    "\n",
    "    # Format\n",
    "    df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "    df_dominant_topic.columns = [\n",
    "        'Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'ProcText', 'OrigText']\n",
    "    df_dominant_topic.head(10)\n",
    "    \n",
    "    ## save results to output\n",
    "    filename = 'topic_sentences'+ PSO[k] + '.csv'\n",
    "    pd.DataFrame(df_dominant_topic).to_csv(os.path.join(res_dir, filename), index=False)\n",
    "\n",
    "    return(df_topic_sents_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(corpus=bow_corpus, cleaned_texts=texts, ldamodel=lda_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Representative sentence for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the most representative sentence for each topic\n",
    "# Display setting to show more characters in column\n",
    "\n",
    "def get_represent_topic_sent(df_topic_sents_keywords):\n",
    "\n",
    "    pd.options.display.max_colwidth = 100\n",
    "\n",
    "    sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "    sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "    for i, grp in sent_topics_outdf_grpd:\n",
    "        sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet,\n",
    "                                                grp.sort_values(['Perc_Contribution'], ascending=False).head(1)],\n",
    "                                                axis=0)\n",
    "\n",
    "    # Reset Index\n",
    "    sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Format\n",
    "    sent_topics_sorteddf_mallet.columns = [\n",
    "        'Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\", \"orig_texts\"]\n",
    "\n",
    "    # Show\n",
    "    sent_topics_sorteddf_mallet\n",
    "\n",
    "    ## save results to output\n",
    "    filename = 'topic_sentence_representatives' + PSO[k] + '.csv'\n",
    "    pd.DataFrame(sent_topics_sorteddf_mallet).to_csv(os.path.join(res_dir, filename), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_represent_topic_sent(df_topic_sents_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word counts and importance of topic words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_topic_word_importance(lda_model, dat_tok_cleaned_test, num_topics):\n",
    "\n",
    "    topics = lda_model.show_topics(formatted=False, num_topics=-1)\n",
    "    data_flat = [w for w_list in dat_tok_cleaned_test for w in w_list]\n",
    "    counter = Counter(data_flat)\n",
    "\n",
    "    out = []\n",
    "    for i, topic in topics:\n",
    "        for word, weight in topic:\n",
    "            out.append([word, i, weight, counter[word]])\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        out, columns=['word', 'topic_id', 'importance', 'word_count'])\n",
    "\n",
    "    # Plot Word Count and Weights of Topic Keywords\n",
    "    num_rows = math.ceil(num_topics/3)\n",
    "    fig, axes = plt.subplots(num_rows, 3, figsize=(16, 10), sharey=True, dpi=160)\n",
    "    col_repeats = plt.cm.get_cmap('tab20').colors\n",
    "    cols = col_repeats #[color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        if(i < num_topics):\n",
    "            ax.bar(x='word', height=\"word_count\",\n",
    "                data=df.loc[df.topic_id == i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n",
    "            ax_twin = ax.twinx()\n",
    "            ax_twin.bar(x='word', height=\"importance\",\n",
    "                        data=df.loc[df.topic_id == i, :], color=cols[i], width=0.2, label='Weights')\n",
    "            ax.set_ylabel('Word Count', color=cols[i])\n",
    "            ax_twin.set_ylim(0, 0.030)\n",
    "            ax.set_ylim(0, 10000)\n",
    "            ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n",
    "            ax.tick_params(axis='y', left=False)\n",
    "            ax.set_xticklabels(df.loc[df.topic_id == i, 'word'],\n",
    "                            rotation=30, horizontalalignment='right')\n",
    "            ax.legend(loc='upper left')\n",
    "            ax_twin.legend(loc='upper right')\n",
    "\n",
    "    fig.tight_layout(w_pad=2)\n",
    "    fig.suptitle('Word Count and Importance of Topic Keywords',\n",
    "                fontsize=22, y=1.05)\n",
    "\n",
    "    fig_coh_name =  'Topic_word_count_importance' + PSO[k] + '.png'\n",
    "    print(fig_coh_name)\n",
    "    plt.savefig(res_dir + '/' + fig_coh_name)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_topic_word_importance(lda_model, dat_tok_cleaned_test, num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 T-SNE Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic weights and dominant topics ------------\n",
    "# t-SNE Clustering Chart - visualize the clusters of documents in a 2D space using t-SNE (t-distributed stochastic neighbor embedding) algorithm.\n",
    "from sklearn.manifold import TSNE\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import Label\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "# Get topic weights\n",
    "topic_weights = []\n",
    "for i, row_list in enumerate(lda_model[bow_corpus]):\n",
    "    topic_weights.append([w for i, w in row_list])\n",
    "\n",
    "# Array of topic weights    \n",
    "arr = pd.DataFrame(topic_weights).fillna(0).values\n",
    "\n",
    "# Keep the well separated points (optional)\n",
    "arr = arr[np.amax(arr, axis=1) > 0.35]\n",
    "\n",
    "# Dominant topic number in each doc\n",
    "topic_num = np.argmax(arr, axis=1)\n",
    "\n",
    "# tSNE Dimension Reduction\n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n",
    "tsne_lda = tsne_model.fit_transform(arr)\n",
    "\n",
    "# Plot the Topic Clusters using Bokeh\n",
    "output_notebook()\n",
    "n_topics = 6\n",
    "mycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\n",
    "plot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), \n",
    "              plot_width=900, plot_height=700)\n",
    "plot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models\n",
    "import pickle\n",
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "p = pyLDAvis.gensim_models.prepare(lda_model, bow_corpus, dic_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(p, 'lda_test.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.LLMs (Run on Google Colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 OpenAI Classification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file from Google Drive\n",
    "df = pd.read_csv('/content/drive/MyDrive/uchicago/NLP/data/final_df_withtoken.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First try using sklearn to identify benefits and challenges in the df\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define keywords for benefits and challenges\n",
    "benefits_keywords = ['access to care', 'efficiency', 'effective','cost-effective',\"accessibility\", \"convenience\",\"cheap\"]\n",
    "challenges_keywords = ['concern', 'difficulties', 'limited',\"technical issues\", \"challenge\",'difficult','expensive','lack','risk',\"breach\"]\n",
    "\n",
    "# Initialize CountVectorizer to transform texts into binary variables based on keywords\n",
    "vectorizer = CountVectorizer(vocabulary=benefits_keywords + challenges_keywords, binary=True)\n",
    "keyword_features = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# Convert to DataFrame and concatenate with the original DataFrame\n",
    "keywords_df = pd.DataFrame(keyword_features.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df = pd.concat([df, keywords_df], axis=1)\n",
    "\n",
    "# Create binary variables for each set of keywords (1 if any keyword is present, 0 otherwise)\n",
    "df['benefits'] = df[benefits_keywords].sum(axis=1) > 0\n",
    "df['challenges'] = df[challenges_keywords].sum(axis=1) > 0\n",
    "\n",
    "# Create a treatment variable based on the presence of benefits vs. challenges keywords\n",
    "df['treatment_focus'] = np.where(df['benefits'], 1,\n",
    "                                               np.where(df['challenges'], -1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.treatment_focus.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_text(text, max_length=4097):\n",
    "    \"\"\"Truncate text to ensure it fits within a specified maximum length.\"\"\"\n",
    "    return text if len(text) <= max_length else text[:max_length]\n",
    "\n",
    "def classify_text_with_openai(text):\n",
    "    truncated_text = truncate_text(text, max_length=4097)  # Adjust max_length as needed\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Is this text discussing the benefits or challenges of telehealth? Return only one word, 'benefit' or 'challenges' or 'neither', Text: '{truncated_text}'\"}\n",
    "        ]\n",
    "    )\n",
    "    # Correctly access the response data\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# Apply the classification function to each text in your sample\n",
    "df_subset['openai_classification'] = df_subset['text'].apply(classify_text_with_openai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset['openai_classification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_subset.to_csv('/content/drive/MyDrive/uchicago/NLP/data/df_subset.csv', index=False)\n",
    "df=pd.read_csv('/content/drive/MyDrive/uchicago/NLP/data/df_subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['openai_classification'] = df['openai_classification'].apply(lambda x: x.lower())\n",
    "df['openai_classification'] = df['openai_classification'].apply(lambda x: 0 if x == 'neither' else 1 if x == 'benefit' else -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.openai_classification.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def evaluate_classification_multiclass(true_labels, predictions, method_name):\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted') #macro\n",
    "\n",
    "    print(f\"Results for {method_name}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision (Weighted Avg.): {precision:.4f}\")\n",
    "    print(f\"Recall (Weighted Avg.): {recall:.4f}\")\n",
    "    print(f\"F1 Score (Weighted Avg.): {f1:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_classification_multiclass(df['true_label'], df['openai_classification'], 'OpenAI API Classification')\n",
    "evaluate_classification_multiclass(df['true_label'], df['treatment_focus'], 'Transformer Classification')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model from Hugging Face hub\n",
    "base_model = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# New instruction dataset\n",
    "guanaco_dataset = \"mlabonne/guanaco-llama2-1k\"\n",
    "\n",
    "# Fine-tuned model\n",
    "new_model = \"llama-2-7b-chat-guanaco\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Comparing Fine-tune LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change base model ?\n",
    "# shorten text\n",
    "\n",
    "dataset = load_dataset('csv', data_files='/content/drive/MyDrive/uchicago/NLP/data/df_subset_short.csv',split=\"train\")\n",
    "# Enable GPU training if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_args = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=32, # the rank of the adaptation\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_params = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=25,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=True, # speed up\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_args,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=None,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_params,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Training parameters with mixed precision (fp16) enabled\n",
    "training_params = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,  # Adjust based on your GPU memory and desired effective batch size\n",
    "    optim=\"adamw_torch\",  # Consider using the default optimizer for simplicity\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    save_total_limit=1,  # Limit the number of saved model checkpoints\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_params,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the LLM\n",
    "dataset = load_dataset('csv', data_files='/content/drive/MyDrive/uchicago/NLP/data/df_subset.csv',split=\"train\")\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "peft_args = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=32,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "training_params = TrainingArguments(\n",
    "    output_dir=\"results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=25,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_args,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=None,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_params,\n",
    "    packing=False,\n",
    ")\n",
    "trainer.train()\n",
    "model.save_pretrained(new_model)\n",
    "tokenizer.save_pretrained(new_model)\n",
    "\n",
    "# Evaluate the performance of the OpenAI LLM, the fine-tuned LLM, and the \"vanilla\" un-tuned LLM\n",
    "openai_results = evaluate_openai_llm(dataset)\n",
    "vanilla_results = evaluate_vanilla_llm(dataset)\n",
    "fine_tuned_results = evaluate_fine_tuned_llm(dataset)\n",
    "\n",
    "# Compare the performance of the models\n",
    "print(\"OpenAI LLM:\")\n",
    "print(openai_results)\n",
    "print(\"Vanilla LLM:\")\n",
    "print(vanilla_results)\n",
    "print(\"Fine-tuned LLM:\")\n",
    "print(fine_tuned_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Synthesize Data with LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/MyDrive/uchicago/NLP/data/df_subset.csv')\n",
    "# Define the prompt template for generating telehealth-related discussions\n",
    "def telehealth_language_nuance_prompt() -> list:\n",
    "    system_message = SystemMessagePromptTemplate(\n",
    "        prompt=PromptTemplate(\n",
    "            input_variables=[],\n",
    "            template=\"You are a model trained on telehealth discussions, generating texts that reflect the nuances in telehealth discourse.\"\n",
    "        )\n",
    "    )\n",
    "    human_message = HumanMessagePromptTemplate(\n",
    "        prompt=PromptTemplate(\n",
    "            input_variables=[\"text\", \"num_generations\"],\n",
    "            template=\"\"\"Rewrite the following text {num_generations} times, keeping the focus on telehealth. \n",
    "            Aim to highlight aspects such as accessibility, efficiency, and patient satisfaction. \n",
    "            Text: {text}\n",
    "            \"\"\"\n",
    "        )\n",
    "    )\n",
    "    return [system_message, human_message]\n",
    "\n",
    "OPENAI_API_KEY = ''  # Initialize the LLM with API key\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.9, max_tokens=512, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Function to generate augmented texts based on the telehealth discussions\n",
    "def generate_augmented_texts(df):\n",
    "    generated_texts = []\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        text = row[\"text\"]\n",
    "        prompt = ChatPromptTemplate.from_messages(telehealth_language_nuance_prompt())\n",
    "        chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "        augmented_text = chain.run({\"text\": text, \"num_generations\": 1})\n",
    "        generated_texts.append(augmented_text)\n",
    "\n",
    "    df[\"augmented_text\"] = generated_texts\n",
    "    return df\n",
    "\n",
    "# Apply the function to generate augmented texts\n",
    "df_augmented = generate_augmented_texts(df)\n",
    "\n",
    "# Example analysis: Comparing the frequency of the word \"efficiency\" in model-generated vs. actual data\n",
    "efficiency_synthetic = len([k for k in df_augmented[\"augmented_text\"] if \"efficiency\" in k.lower()]) / len(df_augmented[\"augmented_text\"])\n",
    "efficiency_real = len([k for k in df[\"text\"] if \"efficiency\" in k.lower()]) / len(df[\"text\"])\n",
    "\n",
    "print(f\"'Efficiency' present in {round(efficiency_synthetic, 3) * 100}% of synthetic texts\")\n",
    "print(f\"'Efficiency' present in {round(efficiency_real, 3) * 100}% of real texts\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Few-Shots Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Initialize the OpenAI model\n",
    "model = OpenAI(model=\"gpt-3.5-turbo-instruct\", openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Prepare the prompt with the DataFrame information included\n",
    "prediction_prompt = \"\"\"\n",
    "Given some data on news about telehealth, examples are as follows:\n",
    "1.One patient succumbs to brain stroke every four minutes in India says expert Brain stroke is the second most common cause of death in India with one patient succumbing to the disease every four minutes , a top health expert flagged on Thursday . Advertisement Padma Shri awardee Dr ( Prof ) MV Padma Srivastava , who is the most renowned neurologist in the country and is a Professor of Neurology at All India Institute of Medical Sciences , participated in the celebration of International Women 's Day event at Sir Ganga Ram Hospital today . Delivering a keynote address at the event titled , \" Stroke care and its primary preventive methods in poor resource settings in India \" , Dr Srivastava said , \" Stroke is the second most common cause of death in India . About 1,85,000 strokes occur every year in India with nearly one stroke every 40 seconds and one stroke death every 4 minutes . \" She further referred to the Global Burden of Diseases ( GBD ) and said that most incidents of stroke were recorded in burden of stroke with 68.6 per cent incidence of stroke . 70.9 per cent stroke deaths and 77.7 Disability Adjusted Life Years ( DALYs ) lost . These figures are alarming for India with many living in poor resource settings . Another alarming and important finding of the GBD 2010 stroke project is 5.2 million ( 31 per cent ) strokes were in children aged less than 20 years . The stroke burden is greater in India and more so among younger and middleaged people , \" she said . Advertisement The health expert flagged the lack of necessary infrastructure to deal with the alarming data in the country . \" In spite of these alarming figures , many Indian hospitals lack the necessary infrastructure and organization required to treat stroke patients quickly and efficiently and do not deliver adequate stroke care . The stroke services across the country especially in public sector hospitals are deficient in many aspects , \" Dr Srivastava said . The Padma Shri awardee further listed the solutions to boost the infrastructure needed to tackle the rising numbers . \" poor resource settings in India is to adopt Telestroke models in poor resource settings . Implementation of Telemedicine / Telestroke facilities is an important step for bridging the economically and geographically challenged and underprivileged sections of the society , \" she said . This program also included inspirational talks by three distinguished faculty members of the hospital . Advertisement The members included Dr Jayashree Sood , Chairperson , Institute of Anesthesiology , who spoke on how to maintain the balance between work and life , especially for women , Prof Kusum Verma , Advisor Cytopathology who spoke on her experiences mitigating professional challenges and Padma Bhushan Dr Neelam Kler Chairperson of Department of Neonatology who talked about her belief in the saying ' Never say Never ' . This program was hosted by the department of Research and chaired by Prof NK Ganguly , former director general ICMR and Chairperson department of Research , Sir Ganga Ram Hospital . The department of research at Sir Ganga Ram Hospital has a robust PhD program and a state of art equipped laboratory with a focus on basic biology , stem cell biology , immunology , autoimmune and infectious diseases .\n",
    "2.The Wearable Technologies Improving Life for People With Parkinson 's Disease Wearable health technologies are vastly popular with people wanting to improve their physical and mental health . Everything from exercise , sleep patterns , calories consumed and heart rhythms can be tracked by a wearable device . But timely and accurate data is also especially valuable for doctors treating patients with complicated health conditions using virtual care . A new study from the Southern Medical Program ( SMP ) , based at UBC Okanagan , has examined the use of wearable health technology and telehealth to treat patients with Parkinson 's disease . \" Even prior to the pandemic , telehealth helped deliver specialized care to patients living in remote and rural settings , \" says Wile , a clinical investigator with the Centre for Chronic Disease Prevention and Management . \" But with the complex nature of Parkinson 's , we wanted to enhance these appointments to better understand how movements vary throughout a patient 's entire day . \" To add a new layer of health information , Wile and the \" We recruited Parkinson 's patients with either tremors or involuntary movements , \" says Joshua Yoneda , SMP student and coauthor of the study . \" We then divided them into two groups some using telehealth and devicebased health tracking and others attending traditional facetoface appointments . \" The telehealth group wore wearable devices to track their movements , involuntary or not , throughout waking hours . The reported data was then reviewed during telehealth appointments to identify peak times patients experienced Parkinson 's symptoms . \" With the integration of accurate and reliable data from wearable devices , we were able to tailor a patient 's medication to better manage their symptoms throughout the day , \" adds Wile . As part of the study , patients were asked a series of questions from the standardized Parkinson Disease Quality of Life Index . Both study groups were assessed at intervals of six weeks , three months and six months . Overall , the patients using the wearable devices reported positive experiences and health outcomes in combination with telehealth appointments to access specialized care leverage multiple technologies to improve a patient 's quality of life and limit the added stress and cost associated with travel , \" says Yoneda . Reference : Peacock D , Yoneda J , Thomson V , Wile D. Tailoring the use of wearable systems and telehealth for Parkinson 's disease . Parkinsonism & Related Disorders . 2021 ; 89:111112. **40;409;TOOLONG article has been republished from the following materials . Note : material may have been edited for length and content . For further information , please contact the cited source .\n",
    "3.Building smarter healthcare in HK with intelligent automation That Hong Kong 's healthcare system had been under enormous strain is no secret . By the end of March , the territory 's COVID19 numbers had peaked at more than 55,000 infections . Set against the city 's ratio of two doctors per 1,000 patients , far below the ratio in Singapore and Japan ( both at 2.5 ) , the UK ( 3.0 ) and Australia ( 3.8 ) , it becomes obvious that , all things remaining constantan aging population , continued shortage of doctors and nurses and rising costsHong Kong 's buckling healthcare system will need to deliver healthcare services in much smarter and more efficient ways . These trends are reflected in the Hong Kong Hospital Authority 's ( HA ) Strategic Plan 20222027 , which lays out its vision for improving financial stability to \" meet escalating service needs \" through the development of smart care and smart hospitals , the nurturing of a smart workforce , and enhancing service supply . As implied in the strategic plan , digital solutions underlying by intelligent automation ( IA ) and robotics to further develop and improve services already in play , like telemedicine and telecare , but made smarter and efficient for the long haul . How IA can be an enabler Intelligent automation has been utilised by hospitals to resolve their common problems for a long time . IA enables hospitals and clinics to bring in digital workers AIfuelled software designed to model human roles to execute rulesbased tasks such as appointment bookings and referrals , which go a long way in improving operational excellence and the patient experience . The National University Health System ( NUHS ) in Singapore is a great example of how IA is used to automate processes and improve overall operations . In 2018 , NUHS started to automate their backoffice functions , including claims processing and billing . NUHS has to complete approximately 40,000 bill adjustment requests each year , and through IA , they went from spending an average of three to four days turnaround for task completion to immediate turnaround . The use of IA helped NUHS process 75 cashflow contributing to an estimated US$350,000 savings across three years . Patients seeking reimbursement from their insurers and employers also enjoyed better experience . At the height of the pandemic in April 2020 , Singapore health authorities had to set up testing efforts , particularly at foreign worker dormitories , where more than 1,000 COVID19 tests were administered each day . It was a pressing period when the pandemic was spreading fast , and NUHS was under great pressure to register , test , and share the results quickly across the network . NUHS needed to speed up this process . NUHS leveraged IA in the form of digital workers to improve the efficiency of this laborious process . The results were immediate as test registration time was reduced from two minutes to 30 seconds per test , saving NUHS 18 hours each day . Lab results also arrived more quickly , enabling NUHS to process more than 27,000 patients daily . In addition , as a measure to contain the spread of COVID19 and relieve the load on the stretched healthcare workforce , NUHS leveraged on IA more selfserved and empowered to take charge of their own health through smart portal and remote consultation . Heralding a new era of healthcare The global pandemic has opened avenues and accelerated new ways of working and operating in the healthcare sector . In a Blue Prism survey of more than 400 senior level healthcare professionals across the globe , 93 percent said that automation of processes accelerated because of COVID19 , with 58 percent of respondents replacing paper documents with electronic equivalents , and 57 percent taking the opportunity to build new , automated processes that improved the way they interact with patients and other departments . Almost half ( 45 percent ) said they have replaced inperson consultations with video conferencing , a practice that is likely to continue in the years ahead . In the UK , the vast majority of facetoface consultations that were previously carried out daily shifted to remote consultations . The Royal College of General Practitioners revealed data corroborating this trend , with 71 percent of routine consultations being conducted remotely in the four week leading to mid April in prior , 71 percent of patient meetings were facetoface consultations . Looking ahead , IA will play an increasingly greater role , with machine learning algorithms used to assess patient symptoms , as well as their behaviour , language and expressions . Collaboration technologies will also evolve so patient engagements are more natural and clinicians can pick up on nuances in interactions . Clearly , the future is bright for the adoption of IA and digital solutions , with the healthcare sector a fertile ground for further deployments to drive greater sustainable healthcare for all . For its part , the Hong Kong Government has put special emphasis on healthcare technology with the setting up of InnoHK , a major initiative that houses 16 laboratories for global research collaboration , which also includes innovation in areas like artificial intelligence and robotics technologies . More than ever , healthcare organizations are challenged to do more for patients with the same resources or less . By leveraging IA to power laborious and mundane processes while improving existing solutions like telehealth , it is possible for Hong Kong 's healthcare issues around service availability so that doctors and nurses can finally be freed up to focus and care for patients with urgent medical needs .\n",
    "4.Access to the abortion pill mifepristone must be restricted , a U.S. appeals court ruled on Wednesday , ordering a ban on telemedicine prescriptions and shipments of the drug by mail , though the decision will not immediately take effect . A Georgia state law against racketeering could be a powerful tool in prosecuting Donald Trump , but applying charges traditionally used to take down organized crime risks miring the case in legal and logistical complications . A campaign aide to embattled Republican U.S. Representative George Santos was charged with identity theft and wire fraud in federal court for impersonating a top congressional staffer in fundraising appeals , court documents unsealed on Wednesday showed . Stay Informed Information you can trust Reuters , the news and media division of Thomson Reuters , is the world 's largest multimedia news provider , reaching billions of people worldwide every day . Reuters provides business , financial , national and international news to professionals via desktop terminals , the world 's media organizations , industry events and directly to consumers .\n",
    "\n",
    "here are two predictions:\n",
    "\n",
    "1. Prediction 1 forecasts an increasing emphasis on patient privacy and data security in telehealth.\n",
    "2. Prediction 2 forecasts the continued growth and enhancement of telehealth offerings in mental health care and chronic disease management.\n",
    "\n",
    "[Your Task]\n",
    "Considering these given example news, validate if my the two predictions are correct, and make another prediction about the future of telehealth based on the data provided.\n",
    "\"\"\"\n",
    "\n",
    "# Use the model to generate a prediction\n",
    "response = model.generate([few_shot_prompt + prediction_prompt])\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Sentiment Over Years using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class TelehealthResearchLLM:\n",
    "    def __init__(self, api_key, data):\n",
    "        self.api_key = api_key\n",
    "        self.data = data\n",
    "        openai.api_key = self.api_key\n",
    "    \n",
    "    def generate_group_specific_insights(self, country, year):\n",
    "        \"\"\"Generate insights on telehealth services' impact and online discourse for a specific country and year.\"\"\"\n",
    "        country_data = self.data[(self.data['country'].str.lower() == country.lower()) & (self.data['year'] == year)]\n",
    "        texts = country_data['text'].tolist()\n",
    "        prompt = f\"Analyze the following telehealth related discussions from {country} in {year} for linguistic representation, sentiment towards telehealth services, and the impact of telehealth on diagnosis and treatment outcomes. Summarize the key findings.\\n\\nDiscussions:\\n\\n\" + \"\\n\\n\".join(texts[:5])  # Limiting to first 5 for brevity\n",
    "        \n",
    "        try:\n",
    "            client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "            response = client.completions.create(\n",
    "                model=\"gpt-3.5-turbo-instruct\",  # Use \"gpt-4\" if available\n",
    "                prompt=prompt,\n",
    "                temperature=0.7,\n",
    "                max_tokens=500\n",
    "            )\n",
    "            return response.choices[0].text.strip()\n",
    "        except Exception as e:\n",
    "            return str(e)\n",
    "    \n",
    "    def analyze_and_compare(self, countries, year):\n",
    "        \"\"\"Compare telehealth discourse across different countries for a specific year.\"\"\"\n",
    "        insights = {}\n",
    "        for country in countries:\n",
    "            insights[country] = self.generate_group_specific_insights(country, year)\n",
    "        return insights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#data = pd.read_csv('/content/drive/MyDrive/uchicago/NLP/data/df_subset.csv') \n",
    "\n",
    "api_key = ''\n",
    "research_llm = TelehealthResearchLLM(api_key, data)\n",
    "\n",
    "# Specify countries and year for comparison\n",
    "countries = [\"United States\", \"India\", \"Kenya\", \"Brazil\", \"Germany\"]\n",
    "year = 2020 \n",
    "\n",
    "# Generate and compare insights\n",
    "insights = research_llm.analyze_and_compare(countries, year)\n",
    "print(\"Telehealth Insights:\")\n",
    "for country, insight in insights.items():\n",
    "    print(f\"\\nCountry: {country}\\nInsight: {insight}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
